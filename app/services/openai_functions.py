"""OpenAI function calling setup for web search and news analysis."""

import json
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import structlog
from duckduckgo_search import DDGS

logger = structlog.get_logger()


# Function definitions for OpenAI function calling
web_search_function = {
    'name': 'web_search',
    'description': 'Search the web for recent news about a company',
    'parameters': {
        'type': 'object',
        'properties': {
            'query': {
                'type': 'string',
                'description': 'Search query string'
            },
            'date_range': {
                'type': 'string',
                'description': 'Date range for search (1m, 3m, 6m, 1y)',
                'enum': ['1m', '3m', '6m', '1y']
            },
            'language': {
                'type': 'string',
                'description': 'Language for search results (nl, en)',
                'enum': ['nl', 'en']
            },
            'max_results': {
                'type': 'integer',
                'description': 'Maximum number of search results',
                'minimum': 1,
                'maximum': 20,
                'default': 10
            }
        },
        'required': ['query']
    }
}

content_analysis_function = {
    'name': 'analyze_content',
    'description': 'Analyze news content for sentiment and relevance',
    'parameters': {
        'type': 'object',
        'properties': {
            'content': {
                'type': 'string',
                'description': 'Article content to analyze'
            },
            'company_name': {
                'type': 'string',
                'description': 'Company name for relevance analysis'
            },
            'source': {
                'type': 'string',
                'description': 'Source of the content'
            }
        },
        'required': ['content', 'company_name']
    }
}


class OpenAIFunctionHandler:
    """Handler for OpenAI function calls."""
    
    def __init__(self):
        """Initialize the function handler."""
        self.ddgs = DDGS()
        self.trusted_sources = {
            'nos.nl', 'nu.nl', 'rtlnieuws.nl', 'telegraaf.nl', 'ad.nl',
            'fd.nl', 'parool.nl', 'volkskrant.nl', 'trouw.nl', 'nrc.nl',
            'reuters.com', 'bloomberg.com', 'ft.com', 'wsj.com',
            'cnn.com', 'bbc.com', 'techcrunch.com', 'forbes.com'
        }
        
    async def handle_web_search(self, **kwargs) -> List[Dict[str, Any]]:
        """
        Execute web search query and return structured results.
        
        Args:
            query: Search query string
            date_range: Date range for search results
            language: Language preference
            max_results: Maximum number of results
            
        Returns:
            List of search results with metadata
        """
        query = kwargs.get('query', '')
        date_range = kwargs.get('date_range', '6m')
        language = kwargs.get('language', 'nl')
        max_results = kwargs.get('max_results', 10)
        
        logger.info(
            "Performing web search",
            query=query,
            date_range=date_range,
            language=language,
            max_results=max_results
        )
        
        try:
            # Calculate date range
            end_date = datetime.now()
            if date_range == '1m':
                start_date = end_date - timedelta(days=30)
            elif date_range == '3m':
                start_date = end_date - timedelta(days=90)
            elif date_range == '6m':
                start_date = end_date - timedelta(days=180)
            else:  # 1y
                start_date = end_date - timedelta(days=365)
            
            # Perform search using DuckDuckGo
            search_results = []
            try:
                # Use DuckDuckGo search
                results = list(self.ddgs.text(
                    keywords=query,
                    region=f"{language}-{language}",
                    safesearch='moderate',
                    timelimit=date_range,
                    max_results=max_results
                ))
                
                for result in results:
                    search_results.append({
                        'title': result.get('title', ''),
                        'url': result.get('href', ''),
                        'snippet': result.get('body', ''),
                        'source': self._extract_domain(result.get('href', '')),
                        'date': end_date,  # DuckDuckGo doesn't always provide exact dates
                    })
                    
            except Exception as e:
                logger.warning(f"DuckDuckGo search failed: {e}")
                # Return simulated results for testing
                search_results = self._get_fallback_results(query, start_date, end_date)
            
            # Filter and rank results
            filtered_results = self.filter_relevant_sources(search_results)
            formatted_results = self.format_search_results(filtered_results)
            
            logger.info(
                "Web search completed",
                query=query,
                results_found=len(search_results),
                results_filtered=len(formatted_results)
            )
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Web search failed: {e}", exc_info=True)
            return []
    
    def _extract_domain(self, url: str) -> str:
        """Extract domain from URL."""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return parsed.netloc.lower()
        except Exception:
            return 'unknown'
    
    def _get_fallback_results(
        self, query: str, start_date: datetime, end_date: datetime
    ) -> List[Dict[str, Any]]:
        """Generate fallback search results for testing."""
        company_name = query.split()[0].strip('"')
        
        fallback_results = [
            {
                'title': f'{company_name} rapporteert sterke kwartaalresultaten',
                'url': f'https://fd.nl/bedrijfsleven/1234/{company_name.lower()}-kwartaal',
                'snippet': f'{company_name} heeft sterke financiÃ«le prestaties gerapporteerd met een omzetgroei van 12% jaar-op-jaar. Het bedrijf wijst het succes toe aan digitale transformatie-initiatieven.',
                'source': 'fd.nl',
                'date': end_date - timedelta(days=15),
            },
            {
                'title': f'Sectoranalyse: {company_name} loopt voorop in innovatie',
                'url': f'https://nu.nl/economie/6789/{company_name.lower()}-innovatie',
                'snippet': f'Marktanalisten benadrukken {company_name} als leider in technologische innovatie binnen hun sector. Recente productlanceringen hebben positieve marktreacties ontvangen.',
                'source': 'nu.nl',
                'date': end_date - timedelta(days=32),
            },
            {
                'title': f'{company_name} krijgt te maken met regelgevingsonderzoek',
                'url': f'https://nos.nl/artikel/987654-{company_name.lower()}-onderzoek',
                'snippet': f'Regelgevingsautoriteiten hebben een onderzoek gestart naar de bedrijfspraktijken van {company_name}. Het bedrijf stelt dat het volledig zal meewerken aan het onderzoek.',
                'source': 'nos.nl',
                'date': end_date - timedelta(days=45),
            }
        ]
        
        return [
            result for result in fallback_results
            if start_date <= result['date'] <= end_date
        ]
    
    def filter_relevant_sources(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Filter search results based on source quality and relevance.
        
        Args:
            results: Raw search results
            
        Returns:
            Filtered and prioritized results
        """
        if not results:
            return []
        
        # Prioritize trusted sources
        trusted_results = []
        other_results = []
        
        for result in results:
            source = result.get('source', '').lower()
            is_trusted = any(trusted in source for trusted in self.trusted_sources)
            
            if is_trusted:
                result['trust_score'] = 1.0
                trusted_results.append(result)
            else:
                result['trust_score'] = 0.5
                other_results.append(result)
        
        # Combine results with trusted sources first
        filtered_results = trusted_results + other_results
        
        # Remove duplicates based on similar titles
        unique_results = []
        seen_titles = set()
        
        for result in filtered_results:
            title_key = result.get('title', '').lower()[:50]  # First 50 chars
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_results.append(result)
        
        # Limit results to ensure diversity (max 3 per source)
        source_counts = {}
        final_results = []
        
        for result in unique_results:
            source = result.get('source', '')
            source_count = source_counts.get(source, 0)
            
            if source_count < 3:  # Max 3 articles per source
                source_counts[source] = source_count + 1
                final_results.append(result)
        
        logger.info(
            "Results filtered",
            original_count=len(results),
            trusted_count=len(trusted_results),
            final_count=len(final_results)
        )
        
        return final_results[:15]  # Limit to 15 total results
    
    def format_search_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Format search results for analysis.
        
        Args:
            results: Filtered search results
            
        Returns:
            Formatted results ready for AI analysis
        """
        formatted_results = []
        
        for result in results:
            formatted_result = {
                'title': result.get('title', '').strip(),
                'source': result.get('source', 'Unknown'),
                'url': result.get('url', ''),
                'date': result.get('date', datetime.now()),
                'content': result.get('snippet', '').strip(),
                'trust_score': result.get('trust_score', 0.5),
                'language': self._detect_language(result.get('title', '') + ' ' + result.get('snippet', ''))
            }
            
            # Skip results with insufficient content
            if len(formatted_result['content']) < 50:
                continue
            
            # Clean up content
            formatted_result['content'] = self._clean_content(formatted_result['content'])
            
            formatted_results.append(formatted_result)
        
        return formatted_results
    
    def _detect_language(self, text: str) -> str:
        """Simple language detection."""
        dutch_words = ['de', 'het', 'een', 'van', 'en', 'in', 'op', 'voor', 'met', 'door']
        english_words = ['the', 'and', 'of', 'in', 'to', 'for', 'with', 'on', 'by', 'from']
        
        text_lower = text.lower()
        
        dutch_count = sum(1 for word in dutch_words if f' {word} ' in text_lower)
        english_count = sum(1 for word in english_words if f' {word} ' in text_lower)
        
        return 'nl' if dutch_count > english_count else 'en'
    
    def _clean_content(self, content: str) -> str:
        """Clean and normalize content."""
        import re
        
        # Remove extra whitespace
        content = re.sub(r'\s+', ' ', content)
        
        # Remove HTML entities
        content = content.replace('&amp;', '&')
        content = content.replace('&lt;', '<')
        content = content.replace('&gt;', '>')
        content = content.replace('&quot;', '"')
        content = content.replace('&#39;', "'")
        
        # Remove special characters that might cause issues
        content = re.sub(r'[^\w\s\.,;:!?\-\'\"â¬$%()[\]{}]', ' ', content)
        
        return content.strip()
    
    async def handle_content_analysis(self, **kwargs) -> Dict[str, Any]:
        """
        Analyze content for sentiment and relevance.
        
        Args:
            content: Article content
            company_name: Company name for relevance
            source: Content source
            
        Returns:
            Analysis results
        """
        content = kwargs.get('content', '')
        company_name = kwargs.get('company_name', '')
        source = kwargs.get('source', 'Unknown')
        
        # Simple sentiment analysis
        sentiment_score = self._calculate_sentiment(content)
        
        # Simple relevance scoring
        relevance_score = self._calculate_relevance(content, company_name)
        
        # Extract key phrases
        key_phrases = self._extract_key_phrases(content)
        
        return {
            'sentiment_score': sentiment_score,
            'relevance_score': relevance_score,
            'key_phrases': key_phrases,
            'source_trust': self._get_source_trust(source)
        }
    
    def _calculate_sentiment(self, text: str) -> float:
        """Calculate sentiment score from text."""
        positive_words = [
            'goed', 'groot', 'uitstekend', 'positief', 'succes', 'groei', 'winst', 'award',
            'good', 'great', 'excellent', 'positive', 'success', 'growth', 'profit', 'award'
        ]
        negative_words = [
            'slecht', 'terrible', 'negatief', 'verlies', 'probleem', 'onderzoek', 'boete',
            'bad', 'terrible', 'negative', 'loss', 'problem', 'investigation', 'fine'
        ]
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        total = positive_count + negative_count
        if total == 0:
            return 0.0
        
        sentiment = (positive_count - negative_count) / total
        return max(-1.0, min(1.0, sentiment))
    
    def _calculate_relevance(self, text: str, company_name: str) -> float:
        """Calculate relevance score."""
        if not company_name:
            return 0.0
        
        text_lower = text.lower()
        company_lower = company_name.lower()
        
        # Count mentions
        mentions = text_lower.count(company_lower)
        
        # Basic relevance scoring
        if mentions == 0:
            return 0.0
        elif mentions == 1:
            return 0.6
        elif mentions <= 3:
            return 0.8
        else:
            return 1.0
    
    def _extract_key_phrases(self, text: str) -> List[str]:
        """Extract key phrases from text."""
        import re
        
        # Simple keyword extraction
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        
        # Filter out common words
        stop_words = {
            'de', 'het', 'een', 'van', 'en', 'in', 'op', 'voor', 'met', 'door',
            'the', 'and', 'of', 'in', 'to', 'for', 'with', 'on', 'by', 'from',
            'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had'
        }
        
        meaningful_words = [word for word in words if len(word) > 3 and word not in stop_words]
        
        # Count frequency
        word_counts = {}
        for word in meaningful_words:
            word_counts[word] = word_counts.get(word, 0) + 1
        
        # Return top phrases
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        return [word for word, count in sorted_words[:5]]
    
    def _get_source_trust(self, source: str) -> float:
        """Get trust score for source."""
        source_lower = source.lower()
        is_trusted = any(trusted in source_lower for trusted in self.trusted_sources)
        return 1.0 if is_trusted else 0.5


# Available functions for OpenAI
AVAILABLE_FUNCTIONS = {
    'web_search': web_search_function,
    'analyze_content': content_analysis_function
}

# Function implementations
FUNCTION_HANDLERS = {
    'web_search': OpenAIFunctionHandler().handle_web_search,
    'analyze_content': OpenAIFunctionHandler().handle_content_analysis
}